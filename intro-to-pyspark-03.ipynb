{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Introduction to Spark In-memory Computing via Python PySpark </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash launch_spark_cluster.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pyspark\n",
    "\n",
    "env_spark_home=os.path.join(os.environ['HOME'],\"software\",\"spark-2.4.5-bin-hadoop2.7\")\n",
    "env_spark_conf_dir=os.path.join(env_spark_home,\"conf\")\n",
    "env_pyspark_python=os.path.join(\"/software\",\"anaconda3\",\"5.1.0\",\"bin\",\"python\")\n",
    "\n",
    "os.environ['SPARK_HOME'] = env_spark_home\n",
    "os.environ['SPARK_CONF_DIR'] = env_spark_conf_dir\n",
    "os.environ['PYSPARK_PYTHON'] = env_pyspark_python\n",
    "\n",
    "fp = open(os.path.join(env_spark_conf_dir,\"master\"))\n",
    "node_list = fp.readlines()\n",
    "\n",
    "import pyspark\n",
    "conf = pyspark.SparkConf()\n",
    "conf.setMaster(\"spark://\" + node_list[0].strip() + \":7077\")\n",
    "conf.setAppName('big-data-workshop')\n",
    "conf.set(\"spark.driver.memory\",\"5g\")\n",
    "conf.set(\"spark.executor.instances\", \"2\")\n",
    "conf.set(\"spark.executor.memory\",\"5g\")\n",
    "conf.set(\"spark.executor.cores\",\"5\")\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Airlines Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spark SQL**\n",
    "- Spark module for structured data processing\n",
    "- provides more information about the structure of both the data and the computation being performed for additional optimization\n",
    "- execute SQL queries written using either a basic SQL syntax or HiveQL\n",
    "\n",
    "**DataFrame**\n",
    "- a distributed collection of data organized into named columns\n",
    "- conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood\n",
    "- can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = pyspark.SQLContext(sc)\n",
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines = sqlContext.read.format(\"com.databricks.spark.csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferschema\", \"true\")\\\n",
    "    .load(\"/zfs/citi/airlines/data/\")\\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "airlines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "airlines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can interact with a DataFrame via SQLContext using SQL statements by registering the DataFrame as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines.registerTempTable(\"airlines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How many unique airlines are there?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uniqueAirline = sqlContext.sql(\"SELECT DISTINCT UniqueCarrier \\\n",
    "                                FROM airlines\")\n",
    "uniqueAirline.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Calculate how many flights completed by each carrier over time*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "carrierFlightCount = sqlContext.sql(\"SELECT UniqueCarrier, COUNT(UniqueCarrier) AS FlightCount \\\n",
    "                                    FROM airlines GROUP BY UniqueCarrier\")\n",
    "carrierFlightCount.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How do you display full carrier names?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carriers = sqlContext.read.format(\"com.databricks.spark.csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferschema\", \"true\")\\\n",
    "    .load(\"/zfs/citi/airlines/metadata/carriers.csv\")\\\n",
    "    .cache()\n",
    "carriers.registerTempTable(\"carriers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carriers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "carrierFlightCountFullName = sqlContext.sql(\"SELECT c.Description, a.UniqueCarrier, COUNT(a.UniqueCarrier) AS FlightCount \\\n",
    "                                    FROM airlines AS a \\\n",
    "                                    INNER JOIN carriers AS c \\\n",
    "                                    ON c.Code = a.UniqueCarrier \\\n",
    "                                    GROUP BY a.UniqueCarrier, c.Description \\\n",
    "                                    ORDER BY a.UniqueCarrier\")\n",
    "carrierFlightCountFullName.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What is the averaged departure delay time for each airline?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "avgDepartureDelay = sqlContext.sql(\"SELECT FIRST(c.Description), FIRST(a.UniqueCarrier), AVG(a.DepDelay) AS AvgDepDelay \\\n",
    "                                    FROM airlines AS a \\\n",
    "                                    INNER JOIN carriers AS c \\\n",
    "                                    ON c.Code = a.UniqueCarrier \\\n",
    "                                    GROUP BY a.UniqueCarrier \\\n",
    "                                    ORDER BY a.UniqueCarrier\")\n",
    "avgDepartureDelay.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash stop_spark_cluster.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Anaconda 5.1.0)",
   "language": "python",
   "name": "anaconda3-5.1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
